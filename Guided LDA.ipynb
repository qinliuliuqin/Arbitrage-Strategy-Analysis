{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661b4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import docx\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from textblob import Word\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from gensim.models import TfidfModel\n",
    "from textblob import Word\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import texthero as hero\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import compress\n",
    "from lda import guidedlda as glda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ec882",
   "metadata": {},
   "source": [
    "# Create useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f361003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    text = text[:text.find('References')]\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ',text)\n",
    "    text = re.sub(r'-\\n\\s', '',text)\n",
    "    text = re.sub(r'-\\n','',text)\n",
    "    text = re.sub(r'- \\n\\n','',text)\n",
    "    text = re.sub(r'[0-9]+','',text)\n",
    "    text = re.sub(r'The Review of Financial Studies','',text)\n",
    "    text = re.sub(r'The Journal of Finance','',text)\n",
    "    text = re.sub(r'Journal of Financial Economics','',text)\n",
    "    text = re.sub(r'This content downloaded from \\n�������������... on .*?,  .*?  :: UTC������������� \\n\\nAll use subject to https://about.jstor.org/terms','',text)\n",
    "    text = re.sub(r'%s(.+?)%s'%('\\n\\n\\n\\n\\n\\n ','\\n\\n '),'',text)\n",
    "    text = re.sub(r'%s(.+?)%s'%('\\n\\n ','\\n\\n '),'',text)\n",
    "    text = re.sub(\"\\W\",' ',text)\n",
    "    #text = re.sub(r'[^A-Za-z0-9 -]+','',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea04f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    text = re.sub(r'\\b\\w{{{}}}\\b'.format(1), '',text)\n",
    "    text = re.sub(r'\\b\\w{{{}}}\\b'.format(2), '',text)\n",
    "    text = re.sub(r'\\W',' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87101e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_documents(documents):\n",
    "    documents = [clean_data(x) for x in documents]\n",
    "    text=pd.DataFrame({'documents':documents})\n",
    "    documents_clean = hero.remove_html_tags(text.documents)\n",
    "    documents_clean = hero.clean(text.documents)\n",
    "    documents_clean = documents_clean.apply(clean_str)\n",
    "    documents_clean = hero.remove_whitespace(documents_clean)\n",
    "    \n",
    "    return documents_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc89f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function that transform the raw documents into document-term matrix (term could be any n-gram by choosing ngram_range)\n",
    "def Ngram_frequency(documents,threshold,batch_size=100000,ngram_range=(1,2)):\n",
    "    model = CountVectorizer(tokenizer=casual_tokenize,ngram_range=ngram_range)\n",
    "    docs = model.fit_transform(raw_documents=documents)\n",
    "    L = int(np.floor(docs.shape[1]/batch_size))\n",
    "    bigram_feature_name=model.get_feature_names()\n",
    "    high_freq_index = np.array(True)\n",
    "    for i in range(1,L):\n",
    "        tfidf_matrix = docs[:,((i-1)*batch_size):(i*batch_size)].toarray()\n",
    "        matrix = tfidf_matrix!=0\n",
    "        high_freq = np.sum(matrix,axis=0) > threshold\n",
    "        high_freq_index = np.append(high_freq_index,high_freq)\n",
    "    tfidf_matrix = docs[:,(L*batch_size):].toarray()\n",
    "    matrix = tfidf_matrix!= 0\n",
    "    high_freq = np.sum(matrix,axis=0) > threshold\n",
    "    high_freq_index = np.append(high_freq_index,high_freq)\n",
    "    high_freq_index = high_freq_index[1:]\n",
    "    bigram_matrix = docs[:,high_freq_index].toarray()\n",
    "    bigram_vocabulary = list(compress(bigram_feature_name, high_freq_index))\n",
    "    r = re.compile(\"[a-z]+\")\n",
    "    index = bigram_vocabulary.index(list(filter(r.match, bigram_vocabulary))[0])\n",
    "    bigram_matrix = bigram_matrix[:,index:]\n",
    "    bigram_vocabulary = bigram_vocabulary[index:]\n",
    "    \n",
    "    return bigram_matrix,bigram_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a4dc3",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2327013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file_path = 'C:/Users/brave/OneDrive/Desktop/Summer 2021/data.pkl'\n",
    "with open(pkl_file_path, 'rb') as f:\n",
    "    docs = pickle.load(f, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "290aee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "time_slice = []\n",
    "for i in range(1995,2021):\n",
    "    num_docs=0\n",
    "    year=str(i)\n",
    "    for journal in docs:\n",
    "        for month in docs[journal][year]:\n",
    "            for paper in docs[journal][year][month]:\n",
    "                documents.append(paper)\n",
    "                num_docs+=1\n",
    "    time_slice.append(num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb89e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_clean = clean_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da82985",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_matrix,dtm_vocabulary = Ngram_frequency(documents_clean, threshold=np.floor(0.01*len(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b81a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tuple(dtm_vocabulary)\n",
    "dictionary = dict(zip(dtm_vocabulary, list(range(len(dtm_vocabulary)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ed30882",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [['book market', 'book tomarket'],\n",
    "                   ['earnings ratio','earnings ratios'],\n",
    "                   ['earnings surprise', 'earnings surprises'],\n",
    "                   ['capm beta','capm betas','beta','beta asset','beta market','beta risk','beta stock','beta stocks','betas market','betas portfolios','beta coefficient','beta coefficients'],\n",
    "                   ['accruals','accrual'],\n",
    "                   ['dividend announcements'],\n",
    "                   ['active traders','active trading'],\n",
    "                   ['advertising expenses','advertising expenditures'],\n",
    "                   ['assets growth'],\n",
    "                   ['capital expenditure','capital expenditures'],\n",
    "                   ['cash holding','cash holdings'],\n",
    "                   ['cash flow','cash flows','cash inflow','cash inflows'],\n",
    "                   ['concentrated industries','concentration measure','concentration measures','industry concentration','measure concentration'],\n",
    "                   ['debt issuance','debt issuances','debt issue','debt issued','debt issuers','debt issues','issuance debt'],\n",
    "                   ['earnings announced','earnings announcement','earnings announcements'],\n",
    "                   ['earnings forecast','earnings forecasts'],\n",
    "                   ['eps forecasts'],\n",
    "                   ['fazzari','fazzari hubbard'],\n",
    "                   ['gross profit','gross profitability'],\n",
    "                   ['hml','hml factor','hml factors','hml high','hml mom','hml momentum','hml return'],\n",
    "                   ['idiosyncratic volatility'],\n",
    "                   ['idiosyncratic return','idiosyncratic returns'],\n",
    "                   ['idiosyncratic risk','idiosyncratic risks'],\n",
    "                   ['intangible','intangible assets'],\n",
    "                   ['momentum','momentum effect','momentum effects','momentum factor','momentum factors','momentum investing','momentum portfolio','momentum portfolios','momentum profits','momentum return','momentum returns','momentum strategies','momentum strategy','momentum traders','momentum trading'],\n",
    "                   ['ohlson'],\n",
    "                   ['operating profit','operating profitability','operating profits'],\n",
    "                   ['pension fund','pension funds'],\n",
    "                   ['real estate'],\n",
    "                   ['seasonalities','seasonality'],\n",
    "                   ['smb','smb factor','smb high','smb small','smb value','smbt','smbt hmlt'],\n",
    "                   ['share issuance','share issues'],\n",
    "                   ['share purchases','share repurchase','share repurchases'],\n",
    "                   ['short interest'],\n",
    "                   ['sin'],\n",
    "                   ['systematic risk','systematic risks','systemic risk'],\n",
    "                   ['tail risk'],\n",
    "                   ['tangible','tangibility','tangency portfolio'],\n",
    "                   ['volatility liquidity','liquidity volatility'],\n",
    "                   ['volume market']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18ac7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = glda.GuidedLDA(n_topics=200, n_iter=100, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f0499e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b7bfc5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gross profitability'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-617d900572be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mst\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed_topic_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mseed_topics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'gross profitability'"
     ]
    }
   ],
   "source": [
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[dictionary[word]] = t_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "648f8dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 5862\n",
      "INFO:lda:vocab_size: 66963\n",
      "INFO:lda:n_words: 59285614\n",
      "INFO:lda:n_topics: 200\n",
      "INFO:lda:n_iter: 100\n",
      "INFO:lda:<0> log likelihood: -883987351\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-05f6c8057abb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtm_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\lda\\guidedlda.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, seed_topics, seed_confidence)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \"\"\"\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed_confidence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\lda\\guidedlda.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, seed_topics, seed_confidence)\u001b[0m\n\u001b[0;32m    250\u001b[0m                 \u001b[1;31m# keep track of loglikelihoods for monitoring convergence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloglikelihoods_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sample_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[0mll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloglikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<{}> log likelihood: {:.0f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\lda\\guidedlda.py\u001b[0m in \u001b[0;36m_sample_topics\u001b[1;34m(self, rands)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m         lda._lda._sample_topics(self.WS, self.DS, self.ZS, self.nzw_, self.ndz_, self.nz_,\n\u001b[0m\u001b[0;32m    361\u001b[0m                                             alpha, eta, rands)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(dtm_matrix, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_200_40=[]\n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topics_200_40.append(topic_words.tolist())\n",
    "    print('Topic {}: {}'.format(i, ', '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(topics_200_40).to_csv('C:/Users/brave/OneDrive/Desktop/Comp 755/lda.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
